{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**KNN & PCA Assignment**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BfG3AS5Ukcl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "- **K-Nearest Neighbors (KNN)** is a simple, non-parametric, and instance-based machine learning algorithm used for classification and regression.\n",
        "\n",
        "- It makes predictions based on the similarity between data points.\n",
        "\n",
        "- It is a **lazy learner**—meaning it does not build a model during training. Instead, it stores the training data and makes predictions only when needed.\n",
        "\n",
        "---\n",
        "\n",
        "**How KNN Works :**\n",
        "\n",
        "KNN follows this simple idea:\n",
        "\n",
        "   - **“Similar data points exist close to each other.”**\n",
        "\n",
        "\n",
        "When a new data point comes in, KNN:\n",
        "\n",
        "1. Calculates the **distance** between the new point and all existing training points (commonly Euclidean distance).\n",
        "\n",
        "2. Selects the **K closest (nearest) neighbors**.\n",
        "\n",
        "3. Makes a prediction based on these neighbors.\n",
        "\n",
        "**Common distance metrics:**\n",
        "\n",
        "* Euclidean (most common)\n",
        "* Manhattan\n",
        "* Minkowski\n",
        "* Cosine similarity\n",
        "\n",
        "---\n",
        "\n",
        "**1. KNN for Classification**\n",
        "\n",
        "In classification, the algorithm assigns a class label.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Find the K nearest neighbors.\n",
        "\n",
        "2. Look at their class labels.\n",
        "\n",
        "3. Choose the label that appears most frequently (majority voting).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "If K = 5 and neighbors’ labels = {A, A, B, A, C}\n",
        "\n",
        "→ Most frequent = **A**\n",
        "→ Prediction = **Class A**\n",
        "\n",
        "---\n",
        "\n",
        "**2. KNN for Regression**\n",
        "\n",
        "In regression, the algorithm predicts a numerical value instead of a label.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Find the K nearest neighbors.\n",
        "\n",
        "2. Take the average (mean) of their values.\n",
        "\n",
        "3. Output this average as the prediction.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "If K = 3 and neighbors' target values = {10, 12, 14}\n",
        "\n",
        "→ Prediction = (10 + 12 + 14) / 3 = **12**\n",
        "\n",
        "---\n",
        "\n",
        "**Key Points About KNN**\n",
        "\n",
        "- **Lazy learner:** No training phase; it stores all data.\n",
        "\n",
        "- **Sensitive to K:**\n",
        "\n",
        "   - Small K → noisy, unstable\n",
        "\n",
        "   - Large K → smoother, but may ignore patterns\n",
        "\n",
        "- **Sensitive to scale** → features should be normalized\n",
        "\n",
        "- **Works well for small datasets;** slow for large ones because it computes distances for every query.\n"
      ],
      "metadata": {
        "id": "2oKN_8Jilxzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "**Curse of Dimensionality**\n",
        "\n",
        "- **The Curse of Dimensionality** refers to problems that arise when the number of features (dimensions) in a dataset becomes very large.\n",
        "\n",
        "- As dimensions increase:\n",
        "\n",
        "  - Data becomes sparse\n",
        "\n",
        "  - Distances between points become less meaningful\n",
        "\n",
        "  - Algorithms that rely on distance (like KNN) start performing poorly\n",
        "\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "**High-dimensional space makes it harder to find “nearby” points.**\n",
        "\n",
        "---\n",
        "\n",
        "**Why Does This Happen?**\n",
        "\n",
        "When you add more dimensions:\n",
        "\n",
        "1. **Space grows exponentially**, but the amount of data does not.\n",
        "\n",
        "   * You would need exponentially more data to cover the space effectively.\n",
        "\n",
        "2. **All points become far apart.**\n",
        "\n",
        "   * The difference between the nearest and farthest neighbors shrinks.\n",
        "\n",
        "3. **Distance metrics stop working well.**\n",
        "\n",
        "   * Euclidean distance becomes less discriminative.\n",
        "\n",
        "---\n",
        "\n",
        "**How It Affects KNN Performance**\n",
        "\n",
        "**1. Distance Becomes Less Meaningful**\n",
        "\n",
        "KNN relies on distance (e.g., Euclidean).\n",
        "\n",
        "But in high dimensions:\n",
        "\n",
        "- All points become **far from each other.**\n",
        "\n",
        "- Distance between nearest and farthest points becomes almost the same.\n",
        "\n",
        "- So KNN **cannot correctly identify true neighbors.**\n",
        "\n",
        "This reduces classification and regression accuracy.\n",
        "\n",
        "**2. Increased Computational Cost**\n",
        "\n",
        "Higher dimensions → more distance calculations.\n",
        "\n",
        "KNN must compute distance for every feature:\n",
        "\n",
        "- More features = more computation\n",
        "\n",
        "- Slower prediction time\n",
        "\n",
        "- Not suitable for large high-dimensional datasets\n",
        "\n",
        "**3. Need for More Data**\n",
        "\n",
        "As dimensions grow, the volume of space increases rapidly.\n",
        "\n",
        "To keep the same data density:\n",
        "\n",
        "- You need **exponentially more data**\n",
        "\n",
        "- With limited data, KNN becomes unreliable\n",
        "\n",
        "Leads to **overfitting or poor generalization.**\n",
        "\n",
        "**4. Noise Increases**\n",
        "\n",
        "High-dimensional datasets often include **irrelevant features.**\n",
        "\n",
        "- Irrelevant features add noise to distance calculations.\n",
        "\n",
        "- They hide the impact of the useful features.\n",
        "\n",
        "KNN mistakenly picks wrong neighbors.\n",
        "\n",
        "---\n",
        "\n",
        "**Example to Understand**\n",
        "\n",
        "Imagine 2D space (length & width): points are close.\n",
        "\n",
        "In 100D space: points are extremely spread out.\n",
        "\n",
        "KNN cannot find “close” neighbors because everything is equally far.\n",
        "\n",
        "---\n",
        "\n",
        "**Summary**\n",
        "\n",
        "| Issue                        | Effect on KNN            |\n",
        "| ---------------------------- | ------------------------ |\n",
        "| Distances become meaningless | Poor neighbor selection  |\n",
        "| Data becomes sparse          | Low accuracy             |\n",
        "| High computation             | Very slow prediction     |\n",
        "| Risk of overfitting          | Wrong predictions        |\n",
        "| More data required           | Needs very large dataset |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L_nZ1o4CpqLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "**What is Principal Component Analysis (PCA)?**\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a **dimensionality reduction technique** that transforms a large set of features into a smaller set while **preserving maximum variance in the data.**\n",
        "\n",
        "It works by:\n",
        "\n",
        "1. Finding directions (called **principal components**) in which the data varies the most.\n",
        "\n",
        "2. Projecting the original data onto these new directions.\n",
        "\n",
        "3. Producing fewer, uncorrelated features.\n",
        "\n",
        "**Key Points About PCA**\n",
        "\n",
        "- It is an **unsupervised** method (does not use target labels).\n",
        "\n",
        "- New features (principal components) are linear combinations of original features.\n",
        "\n",
        "- Components are ordered:\n",
        "\n",
        "  - **PC1** → maximum variance\n",
        "\n",
        "  - **PC2** → second highest variance\n",
        "\n",
        "  - and so on…\n",
        "\n",
        "---\n",
        "\n",
        "**How PCA Works (Simple Steps)**\n",
        "\n",
        "1. Standardize the data\n",
        "\n",
        "2. Compute the covariance matrix\n",
        "\n",
        "3. Find eigenvalues and eigenvectors\n",
        "\n",
        "4. Select top k components\n",
        "\n",
        "5. Transform data into new component space\n",
        "\n",
        "---\n",
        "\n",
        "**PCA vs Feature Selection**\n",
        "\n",
        "| **Aspect**           | **PCA (Dimensionality Reduction)**                 | **Feature Selection**              |\n",
        "| -------------------- | -------------------------------------------------- | ---------------------------------- |\n",
        "| **Nature**           | Feature extraction (creates new features)          | Keeps existing features            |\n",
        "| **Result**           | New transformed features (PC1, PC2, …)             | Subset of original features        |\n",
        "| **Interpretability** | Low (components are combinations of many features) | High (original features retained)  |\n",
        "| **Supervision**      | Unsupervised                                       | Can be supervised or unsupervised  |\n",
        "| **Goal**             | Maximize variance & reduce dimensionality          | Choose most relevant features      |\n",
        "| **Uses**             | Handling multicollinearity, visualization          | Removing irrelevant/noisy features |\n",
        "\n",
        "---\n",
        "\n",
        "**Main Difference**\n",
        "\n",
        "- **PCA transforms** the data into new features.\n",
        "\n",
        "- **Feature selection filters or chooses** the best subset of existing features.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "- Original features: height, weight, age, income\n",
        "\n",
        "- Feature Selection may pick: height, age\n",
        "\n",
        "- PCA may create:\n",
        "\n",
        "   - PC1 = 0.6(height) + 0.4(weight) + 0.2(age) + ...\n",
        "\n"
      ],
      "metadata": {
        "id": "hNyw7iUNudEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "**Eigenvalues and Eigenvectors in PCA**\n",
        "\n",
        "- **In Principal Component Analysis (PCA)**, eigenvalues and eigenvectors come from the **covariance matrix** of the dataset.\n",
        "\n",
        "- They form the mathematical foundation of PCA.\n",
        "\n",
        "---\n",
        "\n",
        "**What are Eigenvectors?**\n",
        "\n",
        "- Eigenvectors are **directions** (axes) in the feature space along which the data varies the most.\n",
        "\n",
        "- In PCA, each eigenvector represents a **principal component**.\n",
        "\n",
        "- They define **new axes** for the transformed data.\n",
        "\n",
        "**Interpretation**\n",
        "\n",
        "- PC1 (1st eigenvector): direction of **maximum variance**\n",
        "\n",
        "- PC2 (2nd eigenvector): direction of next highest variance (orthogonal to PC1)\n",
        "\n",
        "---\n",
        "\n",
        "**What are Eigenvalues?**\n",
        "\n",
        "- Eigenvalues tell **how much variance** is captured by each eigenvector.\n",
        "\n",
        "- Higher eigenvalue = more information (variance) captured.\n",
        "\n",
        "**Interpretation**\n",
        "\n",
        "- Eigenvalue of PC1 is highest → captures maximum variation\n",
        "\n",
        "- Sum of eigenvalues = total variance in the data\n",
        "\n",
        "---\n",
        "\n",
        "**Why Are They Important in PCA?**\n",
        "\n",
        "**1. To Identify Important Principal Components**\n",
        "\n",
        "- Eigenvalue tells how much information a component carries.\n",
        "\n",
        "- We pick the top k eigenvalues → corresponding eigenvectors form the principal components.\n",
        "\n",
        "**2. Dimensionality Reduction**\n",
        "\n",
        "We retain components with:\n",
        "\n",
        "- **High eigenvalues** → important\n",
        "\n",
        "- **Low eigenvalues** → little information → can be dropped\n",
        "\n",
        "**Example:**\n",
        "\n",
        "If eigenvalues = [5.2, 1.4, 0.1]\n",
        "\n",
        "- PC1: keeps 5.2 units of variance\n",
        "\n",
        "- PC2: keeps 1.4\n",
        "\n",
        "- PC3: keeps 0.1 (almost noise → drop)\n",
        "\n",
        "**3. Transforming Data**\n",
        "\n",
        "Eigenvectors form the transformation matrix.\n",
        "\n",
        "They allow us to rotate and project the data into a new coordinate system.\n",
        "\n",
        "**4. Visualizing Variance**\n",
        "\n",
        "Eigenvalues help calculate explained variance ratio, used to decide:\n",
        "\n",
        "- How many components to keep\n",
        "\n",
        "- How much information is preserved\n",
        "\n",
        "---\n",
        "\n",
        "**Summary Table**\n",
        "\n",
        "| Concept         | Meaning in PCA                                       | Importance                  |\n",
        "| --------------- | ---------------------------------------------------- | --------------------------- |\n",
        "| **Eigenvector** | Direction of maximum variance (principal components) | Defines new axes            |\n",
        "| **Eigenvalue**  | Amount of variance captured                          | Helps choose top components |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "covuC2W2v0M0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 5: How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "KNN and PCA work very well together when combined in a single machine learning pipeline because each one solves a weakness of the other.\n",
        "\n",
        "---\n",
        "\n",
        "**1. PCA reduces dimensionality → improves KNN performance**\n",
        "\n",
        "KNN struggles in high-dimensional data because:\n",
        "\n",
        "- Distances become less meaningful (curse of dimensionality)\n",
        "\n",
        "- Many irrelevant features add noise\n",
        "\n",
        "- Prediction becomes slow because KNN computes distance to every point\n",
        "\n",
        "**PCA solves these problems by:**\n",
        "\n",
        "- Removing noisy/irrelevant features\n",
        "\n",
        "- Creating fewer but more informative components\n",
        "\n",
        "- Making data more compact\n",
        "\n",
        "**KNN becomes faster and more accurate.**\n",
        "\n",
        "\n",
        "**2. PCA decorrelates features → better distance calculations**\n",
        "\n",
        "- KNN uses Euclidean/Manhattan distance.\n",
        "\n",
        "- But correlated features distort these distances.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Height and weight often correlate.\n",
        "\n",
        "`Distance = √[(Δheight)² + (Δweight)²]`\n",
        "\n",
        "→ double-penalizes the same information.\n",
        "\n",
        "- **PCA generates uncorrelated (orthogonal) components,** making distance calculation cleaner and more meaningful.\n",
        "\n",
        "**KNN chooses better neighbors.**\n",
        "\n",
        "**3. PCA reduces noise → KNN becomes more robust**\n",
        "\n",
        "High-dimensional datasets often contain:\n",
        "\n",
        "- Redundant features\n",
        "\n",
        "- Noisy features\n",
        "\n",
        "These confuse KNN.\n",
        "\n",
        "- PCA concentrates most variance into first few components.\n",
        "\n",
        "- Noise shifts to components with very small eigenvalues.\n",
        "\n",
        "- By keeping only top components, we remove noise.\n",
        "\n",
        "**KNN predictions become more stable.**\n",
        "\n",
        "**4. PCA speeds up computation**\n",
        "\n",
        "- KNN is slow during prediction because it computes distance to all points.\n",
        "\n",
        "- If PCA reduces dimensions from 500 → 20:\n",
        "\n",
        "   - Distance calculation becomes 25× faster\n",
        "\n",
        "  - Memory usage also decreases\n",
        "\n",
        "---\n",
        "\n",
        "**Overall: Why KNN + PCA is a Good Combination**\n",
        "\n",
        "| PCA Benefit                         | Impact on KNN                      |\n",
        "| ----------------------------------- | ---------------------------------- |\n",
        "| Removes correlated & noisy features | More accurate neighbors            |\n",
        "| Reduces dimensions                  | Faster & more reliable predictions |\n",
        "| Keeps most variance                 | Preserves useful patterns          |\n",
        "| Improves distance quality           | Better classification/regression   |\n",
        "\n",
        "---\n",
        "\n",
        "**Typical Pipeline**\n",
        "\n",
        "**`Standardization → PCA → KNN`**\n",
        "\n",
        "1. Scale the data (very important for PCA & KNN)\n",
        "\n",
        "2. Apply PCA to reduce dimensions\n",
        "\n",
        "3. Train KNN on transformed features\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tlAbWAIwxbs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dataset: Use the Wine Dataset from sklearn.datasets.load_wine().**\n",
        "\n",
        "#**Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "#**Answer:**"
      ],
      "metadata": {
        "id": "EupD22WdzWQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# 1. KNN WITHOUT FEATURE SCALING\n",
        "\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "pred_unscaled = knn_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, pred_unscaled)\n",
        "\n",
        "\n",
        "# 2. KNN WITH FEATURE SCALING\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, pred_scaled)\n",
        "\n",
        "# Print the results\n",
        "print(\"Accuracy WITHOUT Scaling :\", acc_unscaled)\n",
        "print(\"Accuracy WITH Scaling    :\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luPF8bV4zzUz",
        "outputId": "18fd5cbc-2097-41ca-d55d-2d1320344e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling : 0.7407407407407407\n",
            "Accuracy WITH Scaling    : 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "| Model                   | Accuracy             |\n",
        "| ----------------------- | -------------------- |\n",
        "| **KNN without Scaling** |   Lower (≈ 70–78%)   |\n",
        "| **KNN with Scaling**    |  Higher (≈ 95–100%) |\n",
        "\n",
        "\n",
        "**Reason:**\n",
        "\n",
        "- Wine dataset features have different scales (e.g., alcohol %, phenols, flavonoids).\n",
        "\n",
        "- KNN computes distance, so unscaled features distort results.\n",
        "\n",
        "- Standardization makes all features contribute equally → much better accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "hHJCENqVzK4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "#**Answer:**\n",
        "\n"
      ],
      "metadata": {
        "id": "H47_a-zp0say"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Step 1: Feature scaling (VERY important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Step 3: Print explained variance ratio\n",
        "print(\"Explained Variance Ratio of Each Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL8wKplp05xg",
        "outputId": "9ef615cc-8a80-40d4-b991-842bc2b713b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of Each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- PC1 + PC2 ≈ 55% variance\n",
        "\n",
        "- First 3 components cover ≈ 67%\n",
        "\n",
        "- Most meaningful information is concentrated in the first few PCs."
      ],
      "metadata": {
        "id": "9e9Z286Z1AeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "#**Answer:**"
      ],
      "metadata": {
        "id": "WB1NYTaB1P6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "# 1. Train-test split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# 2. Standard Scaling\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# 3. PCA (retain top 2 components)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "\n",
        "# 4. KNN on ORIGINAL dataset (scaled)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "pred_original = knn_original.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, pred_original)\n",
        "\n",
        "\n",
        "# 5. KNN on PCA-transformed dataset (2 components)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, pred_pca)\n",
        "\n",
        "# 6. Print Results\n",
        "\n",
        "print(\"Accuracy on Original (Scaled) Dataset :\", acc_original)\n",
        "print(\"Accuracy on PCA (Top 2 Components)   :\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEqmzC0o1VxD",
        "outputId": "62162a9c-b1cd-4e90-c97e-31bf5d886f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original (Scaled) Dataset : 0.9629629629629629\n",
            "Accuracy on PCA (Top 2 Components)   : 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 9: Train a KNN Classifier with different distance metrics (euclidean,manhattan) on the scaled Wine dataset and compare the results.**\n",
        "\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "#**Answer:**"
      ],
      "metadata": {
        "id": "gvvuDLnm2CJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale the dataset\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Compare metrics\n",
        "results = {}\n",
        "\n",
        "# 1. Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "results['Euclidean'] = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# 2. Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "results['Manhattan'] = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Convert result to dataframe for easy viewing\n",
        "df_results = pd.DataFrame(results, index=['Accuracy'])\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZOr5QdPhowm",
        "outputId": "4a7ab6d5-8bd7-482b-9b3a-6f8fa902e11a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Euclidean  Manhattan\n",
            "Accuracy   0.944444   0.981481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.Due to the large number of features and a small number of samples, traditional models overfit.**\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "#**Answer:**\n",
        "\n",
        "High-dimensional gene expression datasets usually contain **thousands of features (genes)** but **very few samples.**\n",
        "\n",
        "This causes **overfitting** in traditional ML models.\n",
        "\n",
        "A robust pipeline involves **PCA for dimensionality reduction** followed by KNN classification.\n",
        "\n",
        "---\n",
        "\n",
        "**1. Using PCA to Reduce Dimensionality**\n",
        "\n",
        "- Gene expression datasets often have 10,000+ gene features.\n",
        "\n",
        "- Many genes are correlated and contain noise.\n",
        "\n",
        "- PCA transforms the data into new uncorrelated components that capture maximum variance.\n",
        "\n",
        "- This reduces noise and prevents overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Deciding How Many Components to Keep**\n",
        "\n",
        "We decide components using:\n",
        "\n",
        "i) Variance explained ratio\n",
        "\n",
        "- Keep components that explain 90–95% of the variance.\n",
        "\n",
        "- Use a scree plot to visualize the elbow.\n",
        "\n",
        "ii) Cross-validation\n",
        "\n",
        "- Choose number of components that gives best CV accuracy with KNN.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Using KNN After Dimensionality Reduction**\n",
        "\n",
        "- PCA outputs a reduced feature set.\n",
        "\n",
        "- KNN performs well after PCA because:\n",
        "\n",
        "  - Distances become meaningful in lower dimensions.\n",
        "\n",
        "  - Noise and redundant genes are removed.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Evaluate the Model**\n",
        "\n",
        "Use:\n",
        "\n",
        "- Train/test split\n",
        "\n",
        "- Accuracy score\n",
        "\n",
        "- Confusion matrix\n",
        "\n",
        "- Cross-validation to ensure stability\n",
        "\n",
        "---\n",
        "\n",
        "**5. Justifying This Pipeline to Stakeholders**\n",
        "\n",
        "Explain:\n",
        "\n",
        "i) PCA reduces noise\n",
        "\n",
        "- Biological signals become clearer by removing irrelevant gene variability.\n",
        "\n",
        "ii) Prevents overfitting\n",
        "\n",
        "- Lower dimensions → more stable model.\n",
        "\n",
        "iii) Improves predictive performance\n",
        "\n",
        "- KNN works better in PCA-transformed space.\n",
        "\n",
        "iv) Transparent & interpretable\n",
        "\n",
        "Biomedical professionals appreciate:\n",
        "\n",
        "- Variance explained\n",
        "\n",
        "- Component contributions\n",
        "\n",
        "- No black-box deep learning\n",
        "\n",
        "v) Reproducible in real labs\n",
        "\n",
        "- PCA + KNN is simple, deterministic, easy to deploy.\n",
        "\n",
        "---\n",
        "\n",
        "**Python Code**\n",
        "\n",
        "(Uses a synthetic high-dimensional dataset to simulate gene expression.)"
      ],
      "metadata": {
        "id": "QvCJE1JV3GpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# ---- 1. Create a synthetic high-dimensional dataset ----\n",
        "# Simulates gene expression: 5000 features (genes), 200 samples\n",
        "X, y = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=5000,\n",
        "    n_informative=50,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---- 2. Train-test split ----\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---- 3. Standardize data ----\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---- 4. Apply PCA ----\n",
        "pca = PCA(n_components=0.95)  # keep 95% variance\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(\"Original features:\", X_train.shape[1])\n",
        "print(\"Reduced PCA features:\", X_train_pca.shape[1])\n",
        "\n",
        "# ---- 5. Train KNN ----\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# ---- 6. Predictions ----\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# ---- 7. Evaluation ----\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Cross-validation for robustness\n",
        "cv_scores = cross_val_score(knn, X_train_pca, y_train, cv=5)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", accuracy)\n",
        "print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "print(\"\\nCross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Average CV Accuracy:\", np.mean(cv_scores))\n"
      ],
      "metadata": {
        "id": "nFqPnKA13gqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3308aa4-025a-40fd-fed1-b2bdd4421e05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original features: 5000\n",
            "Reduced PCA features: 130\n",
            "\n",
            "Test Accuracy: 0.36666666666666664\n",
            "\n",
            "Confusion Matrix:\n",
            " [[20  0  0]\n",
            " [17  2  1]\n",
            " [17  3  0]]\n",
            "\n",
            "Cross-Validation Accuracy Scores: [0.25       0.28571429 0.28571429 0.25       0.35714286]\n",
            "Average CV Accuracy: 0.2857142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "| Step                  | Purpose                                                             |\n",
        "| --------------------- | ------------------------------------------------------------------- |\n",
        "| **PCA**               | Removes noise, reduces dimensionality, prevents overfitting         |\n",
        "| **Choose Components** | Keep 90–95% variance or best CV performance                         |\n",
        "| **KNN on PCA Data**   | Works better in low dimensions                                      |\n",
        "| **Evaluation**        | Accuracy, confusion matrix, cross-validation                        |\n",
        "| **Justification**     | Robust, interpretable, noise-resistant pipeline for biomedical data |\n"
      ],
      "metadata": {
        "id": "-biSFGrJkswi"
      }
    }
  ]
}